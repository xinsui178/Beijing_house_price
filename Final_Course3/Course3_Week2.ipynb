{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Codebase, Model Complexity, Regularizers, and Classification\n",
    "\n",
    "#### Making Meaningful Predictions from Data\n",
    "\n",
    "This week we were introduced to the concepts of Complexity, Regularization, and further discussed Classification. In this notebook we will give examples and reasoning behind Complexity, give an example of why Regularization should be used, and give an example of how to Classify your model using precision, accuracy, and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting up the codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by importing some libraries that we'll need thoughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import string # Some string utilities\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer # Stemming\n",
    "import numpy\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "We are going to continue to use the Amazon Gift Card data for this examples. This data contains a large set of reviews paired with start ratings and and various other pieces of information.\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data\n",
    "This should be familiar if you have taken previous courses on that discuss data ingestion, just remember what type file we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./datasets/amazon_reviews_us_Gift_Card_v1_00.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(path, 'rt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = f.readline()\n",
    "header = header.strip().split('\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the header for the given data along with an entry to show how it relates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO create a list, each element of which is a dictionary of an entry in the review data\n",
    "#TODO cast the categories titled \"'star_rating', 'helpful_votes', and 'total_votes'\" into integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    d['star_rating'] = int(' #TODO ')\n",
    "    d['helpful_votes'] = int(' #TODO ')\n",
    "    d['total_votes'] = int(' #TODO ')\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example entry\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Complexity and Regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next part we will go over two ways of simplifying our dataset (there are plenty more). The next cell will output the length of this dataset. For simplicity sake we will take a smaller portion of this dataset to work with, though all operations below will still apply to the entire dataset, given more time for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab the first 10,000 values of the dataset and put them into a new dataset named shortData\n",
    "\n",
    "shortData = dataset[' #TODO ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check\n",
    "len(shortData) == 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's count the number of unique words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO count the number of unique words found within the 'review_body' portion of your dataset using the .split() function\n",
    "# and the defaultdict collection\n",
    "wordCount = defaultdict(int)\n",
    "\n",
    "#SOLN\n",
    "for d in shortData:\n",
    "    for w in d[' #TODO '].split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "print(len(wordCount)) #Answer should be 11215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this in itself is not too bad to work with, but the actual dataset has roughly 97,000 unique words, so we are still working with a smaller fraction of the data, so let's try and improve this.\n",
    "\n",
    "Next, lets try and reduce the amount of words by removing punctuation and capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountPunc = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "#TODO same as above without use of stemming\n",
    "\n",
    "\n",
    "#SOLN\n",
    "for d in shortData:\n",
    "  r = ''.join([c for c in d[' #TODO '].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCountPunc[w] += 1\n",
    "    \n",
    "print(len(wordCountPunc)) #Answer should be 6023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is better (roughly half the previous amount) but we can do better!\n",
    "\n",
    "\n",
    "Lets build a few data structures to count the number of instances of each word. Here you want to remove punctuation and capitalization, then apply stemming.\n",
    "\n",
    "Stemming is a tool from the NLTK (Natural Language Toolkit) library. Here is the [link](http://www.nltk.org/howto/stem.html) to how this works. (Hint separate the capitalization and punctuation for each review first, then place the separated words into the stemmer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountStem = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)\n",
    "\n",
    "#Note: this will take longer than the previous to run as a result of stemming\n",
    "\n",
    "#TODO count the number of unique words, this time removing capitalization and punctuation, USE STEMMING HERE\n",
    "\n",
    "\n",
    "#SOLN\n",
    "for d in shortData:\n",
    "    r = ''.join([c for c in d[' #TODO '].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        w = stemmer.stem(w) # with stemming\n",
    "        wordCountStem[w] += 1\n",
    "\n",
    "print(len(wordCountStem)) #Answer should be 4666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap \n",
    "\n",
    "Note that each word here will hold a weight in a model, meaning each unique word is a dimension of the model. As we expand our dataset from our smaller portion into the entirety of the data, our model will grow in dimensionality very quickly. This will cause our model to be highly prone to overfitting.\n",
    "Here's a link to a visual model using dogs and cats! [Here](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/#The_curse_of_dimensionality_and_overfitting)\n",
    "\n",
    "Next week you will learn how to address this by implementing a regularizer using the \"Ridge\" Model. This model from sklearn implements a least squares regression model that includes a regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluating Classifiers for Ranking\n",
    "\n",
    "Last week you learned about Classification Diagnostics (accuracy, precision etc). Using a Logistic Regression model, you can use those calculations to evaluate your classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing count values in order to Classify our model's accuracy and precision down below\n",
    "counts = [(wordCountPunc[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can you figure out what this function is doing? \n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat                 #Next week we will discuss what this is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in dataset] #List of our \"features\"\n",
    "y = [d['star_rating'] for d in dataset] #List of all star ratings\n",
    "y_class = [(rating > 3) for rating in y] #List of all ratings higher than 3 stars\n",
    "\n",
    "modelLin = linear_model.LogisticRegression() #Basic Linear Model\n",
    "modelLin.fit(X, y_class);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modelLin.predict(X)\n",
    "correct = predictions == y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct #Think, what does this array tell us at each entry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate accuracy and precision of the Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO calculate the accuracy through any method\n",
    "\n",
    "#SOLN\n",
    "accuracy = ' #TODO '\n",
    "print(\"Accuracy = \" + str(accuracy)) #Hint this should be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we've worked with accuracy, lets move to ranking based on Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's a quick calculation of accuracy through true/false positives/negatives\n",
    "\n",
    "TP = sum([(p and l) for (p,l) in zip(predictions, y_class)])\n",
    "FP = sum([(p and not l) for (p,l) in zip(predictions, y_class)])\n",
    "TN = sum([(not p and not l) for (p,l) in zip(predictions, y_class)])\n",
    "FN = sum([(not p and l) for (p,l) in zip(predictions, y_class)])\n",
    "\n",
    "TFaccuracy = (TP + TN) / (TP + FP + TN + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check\n",
    "TFaccuracy == accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Calculate the precision and recall using the True/False values defined above\n",
    "\n",
    "#SOLN\n",
    "precision = TP / ' #TODO '\n",
    "recall = TP / ' #TODO '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=[True, False,False]\n",
    "sum([not i for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how high our precision and accuracy are. Individually neither of these are difficult to obtain, but it can be difficult to get both at the same time. When they are both high, that indicates a good model!\n",
    "\n",
    "## You're All Done!\n",
    "\n",
    "Next week we will learn about guidelines for the implementation of predictive pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

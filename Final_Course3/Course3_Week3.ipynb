{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Validating Data and Predictive Pipelines\n",
    "#### Making Meaningful Predictions from Data\n",
    "\n",
    "This week we were introduced to the concept of validation and pipelines. In this notebook, we will go over basic training, testing, and validating data and move on to some pipeline practice as shown in lecture videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Training, Validating, Testing\n",
    "1. **Training datasets** form the basis of a model's learning, teaching it how to estimate its output.\n",
    "2. **Validation datasets** check the accuracy, quality, and integrity of a model, as well as fine-tuning any parameters that are not directly optimized.\n",
    "3. **Test datasets** are usually what we are most interested in -- how does the model perform with previously unseen data that reflects real-world cases?\n",
    "\n",
    "To learn more about the difference between these, click here: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "\n",
    "A good model is one that generalizes to new data! If a model performs well on a training set but not on new data, then it is probably **overfitting** the training data.\n",
    "\n",
    "#### \"Theorems\" about these sets\n",
    "\n",
    "* The training error increases as lambda increases\n",
    "* The validation and test error are at least as large as the training error (assuming infinitely large random partitions)\n",
    "* The validation/test error will usually have a “sweet spot” between under- and over-fitting\n",
    "\n",
    "### The Data\n",
    "\n",
    "Unzip the `reviews.json` file in the Week 2 folder. This dataset contains 25,000 Yelp reviews of various businesses. For this notebook, we will be looking at how well various words can predict a business' rating.\n",
    "\n",
    "### Reading the Data\n",
    "First we import a few libraries. Most of these are the same as before, though a few new libraries are included for string processing. \n",
    "\n",
    "Then, specify the path of the file. You may need to change the given path according to your local environment. This should be familiar if you took Course 1 (*Basic Data Ingestion, Processing, and Visualization*) of the Python Data Products specialization already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import string # Some string utilities\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer # Stemming\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./datasets/reviews.json\"\n",
    "f = open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "# Load in JSON file\n",
    "dataset = []\n",
    "for i in range(25000):\n",
    "    dataset.append(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first entry in dataset\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Examine Our Dataset\n",
    "#### Counting Number of Unique Words\n",
    "Before we start analyzing this data, let's see how many unique words we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total unique words\n",
    "wordCount = defaultdict(int)\n",
    "for d in dataset:\n",
    "    for w in d['text'].split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "print(len(wordCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of words! This number of words is too many to deal with (i.e., it would result in a 119,350 dimensional feature vector if used naively). Let's try to reduce this by removing punctuation and capitalization, so that two instances of the same word will be counted as being the same even if punctuated or capitalized differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total unique words, ignoring punctuation and capitalization\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in dataset:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "print(len(wordCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still left with a large number of words, so possibly we can reduce this number of words further if we treat different word inflections (e.g. \"drinks\" vs. \"drinking\") as being instances of the same word, by identifying their word stem (i.e., \"drink\"). This process is called \"stemming\".\n",
    "We perform stemming using a stemmer from the NLTK (Natural Language Toolkit) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total unique words, ignoring punctuation and capitalization and using stemming.\n",
    "# This one might take a bit of time to run -- be patient!\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer() # object doing the stemming\n",
    "for d in dataset:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        w = stemmer.stem(w) # with stemming\n",
    "        wordCount[w] += 1\n",
    "\n",
    "print(len(wordCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting and Building Features from the Most Common Words\n",
    "That's still a lot of words. One way to deal with this is to take the most common words and build features out of those.\n",
    "\n",
    "First we build a few data structures to count the number of instances of each word. Here we remove punctuation and capitalization, but do not apply stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total unique words, ignoring punctuation and capitalization\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in dataset:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having counted the number of instances of each word, we can sort them to find the most common, and build a word index based on these frequencies. For example, the most frequent word will have index 0, the secont most frequent will have index 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse() # Most frequent = index 0\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our Bag-of-Words Feature Representation\n",
    "Now that we have our dictionary of common words, we can now build a feature vector by counting how often each of these words appears in each review. This is called a **\"bag-of-words\" feature representation**. This results in a 1,000 dimensional feature vector (1,001 if we include an offset term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for building bag-of-words\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Build Training Dataset\n",
    "We will build our training set by doing the following:\n",
    "1. Shuffle entries\n",
    "2. Define `X` - matrix of features\n",
    "3. Define `y` - the outcomes\n",
    "4. Perform least squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in dataset] # using the bag-of-words function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d['stars'] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y,rcond=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Visualize Important Words\n",
    "Once the model has finished training, we can examine which are the most positive (or negative) words by looking at the largest (or smallest) corresponding values for theta To do so, let's sort our words based on their corresponding weights from `theta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordWeights = list(zip(theta, words + ['offset']))\n",
    "wordWeights.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most negative words\n",
    "wordWeights[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 most positive words (10th is offset term)\n",
    "wordWeights[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Include Regularizer\n",
    "**Regularization** is the process of penalizing model complexity during training. In this case, though the above model was effective, it was also very high dimensional, and thus may have been prone to **overfitting**. We can try to address this by adding a regularizer to our model.\n",
    "\n",
    "The \"Ridge\" class from `sklearn` implements a least squares regression model (as in our example above) that includes a regularizer. The strength of the regularizer is controlled by the parameter `alpha` (equivalent to `lambda` in the course lectures). Otherwise, fitting the ridge regression model is exactly the same as fitting a regular least squares model! \n",
    "\n",
    "Here is the method call we are using: `model = linear_model.Ridge(1.0, fit_intercept=False)`\n",
    "\n",
    "**Note the two extra parameters:** The first is the regularization strength (`alpha`). The second indicates we do not want the model to fit an intercept (since our feature vector already includes one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(linear_model.Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the coefficients learned by our new regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordWeights = list(zip(theta, words + ['offset']))\n",
    "wordWeights.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most negative words\n",
    "wordWeights[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 most positive words (10th is offset term)\n",
    "wordWeights[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Regression Diagnostics: MSE and R^2 Review\n",
    "Let's step away from programming for a moment. How do we evaluate our regressors? This section should be familiar from Week 1 of this course.\n",
    "\n",
    "### 2a. MSE (Mean Squared Error)\n",
    "What is the MSE (which we have already been using) and its relationship to the R^2 statistic? We start by extracting the predictions from our model and computing their squared differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = [(x-y)**2 for (x,y) in zip(predictions,y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MSE** is just the average (mean) of these squared differences, which we can compute with a few trivial Python commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = sum(differences) / len(differences)\n",
    "print(\"MSE = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. R^2\n",
    "As we saw in the lectures, the **R^2** (and the **FVU**, or \"Fraction of Variance Unexplained\") normalize the **Mean Squared Error** based on the variance of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVU = MSE / numpy.var(y)\n",
    "R2 = 1 - FVU\n",
    "print(\"R2 = \" + str(R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Predictive Pipelines\n",
    "\n",
    "In its most basic form, a pipeline links steps together. In machine learning and data science, pipelines allow you transform data from one representation to another through a series of steps. \n",
    "\n",
    "A lower-level example can be found in Unix, where you can *pipeline* the *output* of some command as an *input* some other command with the `|` (e.g. `cat words.txt | wc -c` will result in the number of characters in `words.txt`). In this notebook, we will implement a regularization pipeline to help us train, test, and validate data, where (as stated before) **regularization** is the process of penalizing model complexity during training.\n",
    "\n",
    "Let's import libraries and read our dataset in again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./datasets/reviews.json\"\n",
    "f = open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "# Load in JSON file\n",
    "dataset = []\n",
    "for i in range(25000):\n",
    "    dataset.append(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Rebuild Common Word Features\n",
    "This is identical to what we did in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "for d in dataset:\n",
    "  r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse() # Highest frequency = index 0\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words function\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Build the Validation Set\n",
    "Again we split our data, but this time we split it into _three_ parts - a training, a validation, and a test component. Recall that validation sets check the accuracy, quality, and integrity of a model, as well as fine-tuning any parameters that are not directly optimized.\n",
    "\n",
    "Let's shuffle the dataset and define `X` and `y` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "X = [feature(d) for d in dataset]\n",
    "y = [d['stars'] for d in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll use half of our data (and labels) for **training**, the next quarter for **validation**, and the final quarter for **testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X)\n",
    "\n",
    "X_train = X[:N//2]\n",
    "X_valid = X[N//2:3*N//4]\n",
    "X_test = X[3*N//4:]\n",
    "\n",
    "y_train = y[:N//2]\n",
    "y_valid = y[N//2:3*N//4]\n",
    "y_test = y[3*N//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine size of data\n",
    "len(X), len(X_train), len(X_valid), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Train the Model\n",
    "Again we'll train a model based on regularized (Ridge) regression. Our MSE function is the same as before, but this time for convience takes an (already trained) model as an input parameter.\n",
    "\n",
    "#### Finally, to implement the pipeline, we:\n",
    "1. Iterate through various values of lambda\n",
    "2. Fit a ridge regression model for each of these values\n",
    "3. Evaluate the performance of this model on the validation set\n",
    "4. Keep track of which model is the best we've seen so far (on the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find Mean Squared Error\n",
    "def MSE(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    differences = [(a-b)**2 for (a,b) in zip(predictions, y)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to keep track of the current best model and MSE\n",
    "bestModel = None \n",
    "bestMSE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and fine-tune model with training and validation sets\n",
    "for lamb in [0.01, 0.1, 1, 10, 100]:\n",
    "    model = linear_model.Ridge(lamb, fit_intercept=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    mseTrain = MSE(model, X_train, y_train)\n",
    "    mseValid = MSE(model, X_valid, y_valid)\n",
    "    \n",
    "    print(\"lambda = \" + str(lamb) + \", training/validation error = \" +\n",
    "          str(mseTrain) + '/' + str(mseValid))\n",
    "    if not bestModel or mseValid < bestMSE:\n",
    "        bestModel = model\n",
    "        bestMSE = mseValid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance of our best model on the **test** set. Note that this is the only time throughout the entire pipeline that we examine the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseTest = MSE(bestModel, X_test, y_test)\n",
    "print(\"test error = \" + str(mseTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## You're all done!\n",
    "You should be familiar with the basics of training, validating, and testing in Python by now. We encourage you to explore further by using your own datasets and thinking about research questions you can answer with pipelines. You will have a chance to show off your skills at the end of this course with your very own project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
